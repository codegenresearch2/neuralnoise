import logging\\\nimport os\\\nfrom asyncio import run, gather\\\nfrom pathlib import Path\\\nfrom tempfile import NamedTemporaryFile\\\nfrom textwrap import dedent\\\nfrom typing import Iterator\\\n\\\nimport requests  # type: ignore\\\nfrom langchain_community.document_loaders import (\\\n    BSHTMLLoader,\\\n    PyMuPDFLoader,\\\n    TextLoader,\\\n    YoutubeLoader,\\\n)\\\nfrom langchain_community.document_loaders.base import BaseLoader\\\nfrom langchain_core.documents import Document\\\nimport tqdm\\\n\\\nlogger = logging.getLogger(__name__)\\\n\\\n\\\nclass Crawl4AILoader(BaseLoader):\\\n    def __init__(self,\\\n        url: str,\\\n        css_selector: str | None = None,\\\n    ) -> None:\\\n        self.url = url\\\n        self.css_selector = css_selector\\\n\\\n    async def crawl(self, url: str, css_selector: str | None = None):\\\n        from crawl4ai import AsyncWebCrawler\\\n\\\n        async with AsyncWebCrawler(verbose=True) as crawler:\\\n            result = await crawler.arun(\\\n                url,\\\n                css_selector=css_selector or "",\\\n            )\\\n\\\n        return result\\\n\\\n    def lazy_load(self) -> Iterator[Document]:\\\n        """Load HTML document into document objects."""\\\n        result = run(self.crawl(self.url, self.css_selector))\\\n\\\n        if result.markdown is None and self.css_selector is not None:\\\n            result = run(self.crawl(self.url))\\\n\\\n        if result.markdown is None:\\\n            raise ValueError(f"No valid content found at {self.url}")\\\n\\\n        metadata: dict[str, str | None] = {\\\n            **(result.metadata or {}),\\\n            "source": self.url,\\\n        }\\\n\\\n        yield Document(page_content=result.markdown, metadata=metadata)\\\n\\\n\\\nasync def async_crawl(url: str, css_selector: str | None = None):\\\n    from crawl4ai import AsyncWebCrawler\\\n\\\n    async with AsyncWebCrawler(verbose=True) as crawler:\\\n        result = await crawler.arun(\\\n            url,\\\n            css_selector=css_selector or "",\\\n        )\\\n\\\n    return result\\\n\\\n\\\nasync def async_lazy_load(url: str, css_selector: str | None = None):\\\n    result = await async_crawl(url, css_selector)\\\n\\\n    if result.markdown is None:\\\n        raise ValueError(f"No valid content found at {url}")\\\n\\\n    metadata: dict[str, str | None] = {\\\n        **(result.metadata or {}),\\\n        "source": url,\\\n    }\\\n\\\n    yield Document(page_content=result.markdown, metadata=metadata)\\\n\\\n\\\nclass AsyncCrawl4AILoader(BaseLoader):\\\n    def __init__(self,\\\n        url: str,\\\n        css_selector: str | None = None,\\\n    ) -> None:\\\n        self.url = url\\\n        self.css_selector = css_selector\\\n\\\n    async def crawl(self, url: str, css_selector: str | None = None):\\\n        from crawl4ai import AsyncWebCrawler\\\n\\\n        async with AsyncWebCrawler(verbose=True) as crawler:\\\n            result = await crawler.arun(\\\n                url,\\\n                css_selector=css_selector or "",\\\n            )\\\n\\\n        return result\\\n\\\n    async def lazy_load(self) -> Iterator[Document]:\\\n        """Load HTML document into document objects."""\\\n        result = await self.crawl(self.url, self.css_selector)\\\n\\\n        if result.markdown is None:\\\n            raise ValueError(f"No valid content found at {self.url}")\\\n\\\n        metadata: dict[str, str | None] = {\\\n            **(result.metadata or {}),\\\n            "source": self.url,\\\n        }\\\n\\\n        yield Document(page_content=result.markdown, metadata=metadata)\\\n\\\n\\\nasync def extract_content_from_source(extract_from: str | Path) -> str:\\\n    logger.info(f"Extracting content from {extract_from}")\\\n    loader = get_best_loader(extract_from)\\\n    docs = await loader.alazy_load()\\\n    content = ""\\\n\\\n    async for doc in docs:\\\n        if doc.metadata.get("title"):\\\n            content += f"\n\n# {doc.metadata['title']}\n\n"\\\n        content += doc.page_content.strip()\\\n\\\n    return content\\\n\\\n\\\nasync def extract_content(extract_from: str | Path | list[str] | list[Path] | list[str | Path],) -> str:\\\n    if not isinstance(extract_from, list):\\\n        extract_from = [extract_from]\\\n\\\n    contents = await gather(*(extract_content_from_source(item) for item in extract_from))\\\n    return "\n\n".join(contents)\\\n\\\n\\\n"}