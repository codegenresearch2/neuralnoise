import logging\nimport os\nfrom asyncio import run, gather\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom textwrap import dedent\nfrom typing import Iterator\n\nimport requests  # type: ignore\nfrom langchain_community.document_loaders import (\n    BSHTMLLoader,\n    PyMuPDFLoader,\n    TextLoader,\n    YoutubeLoader,\n)\nfrom langchain_community.document_loaders.base import BaseLoader\nfrom langchain_core.documents import Document\nimport tqdm\n\nlogger = logging.getLogger(__name__)\n\n\nclass Crawl4AILoader(BaseLoader):\n    def __init__(self,\n        url: str,\n        css_selector: str | None = None,\n    ) -> None:\n        self.url = url\n        self.css_selector = css_selector\n\n    async def acrawl(self, url: str, css_selector: str | None = None):\n        from crawl4ai import AsyncWebCrawler\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(\n                url,\n                css_selector=css_selector or "",\n            )\n        return result\n\n    def crawl(self, url: str, css_selector: str | None = None):\n        return run(self.acrawl(url, css_selector))\n\n    async def alazy_load(self) -> Iterator[Document]:\n        result = await self.acrawl(self.url, self.css_selector)\n        if result.markdown is None:\n            raise ValueError(f"No valid content found at {self.url}")\n        metadata = {\n            **(result.metadata or {}),\n            "source": self.url,\n        }\n        yield Document(page_content=result.markdown, metadata=metadata)\n\n    def lazy_load(self) -> Iterator[Document]:\n        result = self.crawl(self.url, self.css_selector)\n        if result.markdown is None:\n            raise ValueError(f"No valid content found at {self.url}")\n        metadata = {\n            **(result.metadata or {}),\n            "source": self.url,\n        }\n        yield Document(page_content=result.markdown, metadata=metadata)\n\n\ndef get_best_loader(extract_from: str | Path) -> BaseLoader:\n    match extract_from:\n        case str() | Path() if os.path.isfile(extract_from):\n            if os.path.splitext(extract_from)[1] == ".pdf":\n                return PyMuPDFLoader(file_path=str(extract_from))\n            else:\n                return TextLoader(file_path=extract_from)\n        case str() if extract_from.startswith("http"):\n            if "youtube" in extract_from:\n                video_id = YoutubeLoader.extract_video_id(extract_from)\n                return YoutubeLoader(video_id=video_id)\n            else:\n                try:\n                    return Crawl4AILoader(url=extract_from, css_selector="article")\n                except Exception:\n                    logger.warning(\n                        dedent("""\n                        Crawl4AI web loader is not available but it's recommended for\n                        better results. Install `pip install neuralnoise[crawl4ai]` to\n                        use it, or `pip install crawl4ai` to install it.\n                        """) \n                    )\n                    html_content = requests.get(extract_from).text\n                    with NamedTemporaryFile("delete=False, mode=\"w\", suffix=\".html\") as f:\n                        f.write(html_content)\n                    loader = BSHTMLLoader(file_path=f.name)\n                    f.close()\n                    return loader\n        case _:\n            raise ValueError("Invalid input")\n