import logging\nimport os\nfrom asyncio import run\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom textwrap import dedent\nfrom typing import Iterator\n\nimport requests  # type: ignore\nfrom langchain_community.document_loaders import (\n    BSHTMLLoader,\n    PyMuPDFLoader,\n    TextLoader,\n    YoutubeLoader,\n)\nfrom langchain_community.document_loaders.base import BaseLoader\nfrom langchain_core.documents import Document\nimport tqdm\n\nlogger = logging.getLogger(__name__)\n\n\nclass Crawl4AILoader(BaseLoader):\n    def __init__(self,\n        url: str,\n        css_selector: str | None = None,\n    ) -> None:\n        self.url = url\n        self.css_selector = css_selector\n\n    async def crawl(self, url: str, css_selector: str | None = None):\n        from crawl4ai import AsyncWebCrawler\n\n        async with AsyncWebCrawler(verbose=True) as crawler:\n            result = await crawler.arun(url,\n                css_selector=css_selector or "",\n            )\n\n        return result\n\n    def lazy_load(self) -> Iterator[Document]:\n        \"\"\Load HTML document into document objects.\"\"\n        result = run(self.crawl(self.url, self.css_selector))\n\n        if result.markdown is None and self.css_selector is not None:\n            result = run(self.crawl(self.url))\n\n        if result.markdown is None:\n            raise ValueError(f\"No valid content found at {self.url}\")\n\n        metadata: dict[str, str | None] = {\n            **(result.metadata or {}),\n            "source": self.url,\n        }\n\n        yield Document(page_content=result.markdown, metadata=metadata)\n\n\ndef get_best_loader(extract_from: str | Path) -> BaseLoader:\n    match extract_from:\n        case str() | Path() if os.path.isfile(extract_from):\n            if os.path.splitext(extract_from)[1] == ".pdf":\n                return PyMuPDFLoader(file_path=str(extract_from))\n            else:\n                return TextLoader(file_path=extract_from)\n        case str() if extract_from.startswith("http"):\n            if "youtube" in extract_from:\n                video_id = YoutubeLoader.extract_video_id(extract_from)\n                return YoutubeLoader(video_id=video_id)\n            else:\n                try:\n                    return Crawl4AILoader(url=extract_from, css_selector="article")\n                except Exception:\n                    logger.warning(\n                        dedent(\"\"\"\n                        Crawl4AI web loader is not available but it's recommended for\n                        better results. Install `pip install neuralnoise[crawl4ai]` to\n                        use it, or `pip install crawl4ai` to install it.\n                                    \n                        Once installed, make sure to follow the instructions in their\n                        repo: https://github.com/unclecode/crawl4ai\n                                    \n                        For example, you should run `playwright install` to install\n                        utils for the crawlers to work.\n\n                        Using the default web loader now.\n                    \"\"\"\n                    )\n\n                    html_content = requests.get(extract_from).text\n\n                    with NamedTemporaryFile(\n                        delete=False, mode="w", suffix=".html"\n                    ) as f:\n                        f.write(html_content)\n\n                    loader = BSHTMLLoader(file_path=f.name)\n                    f.close()\n                    return loader\n        case _:\n            raise ValueError("Invalid input")\n\n\ndef extract_content_from_source(extract_from: str | Path) -> str:\n    logger.info(f"Extracting content from {extract_from}")\n    loader = get_best_loader(extract_from)\n    docs = loader.load()\n    content = ""\n\n    for doc in docs:\n        if doc.metadata.get("title"):\n            content += f"\n\n# {doc.metadata['title']}\n\n"\n        content += doc.page_content.strip()\n\n    return content\n\n\ndef extract_content(extract_from: str | Path | list[str] | list[Path] | list[str | Path],) -> str:\n    if not isinstance(extract_from, list):\n        extract_from = [extract_from]\n\n    return "\n\n".join(\n        f"<document>\n{extract_content_from_source(item)}\n</document>"\n        for item in extract_from\n    )\n