import logging\\\\nimport os\\\nfrom pathlib import Path\\\nfrom tempfile import NamedTemporaryFile\\\nfrom textwrap import dedent\\\nfrom typing import Iterator\\\n\\nimport requests  # type: ignore\\\nfrom langchain_community.document_loaders import (  \\\n    BSHTMLLoader,\\\n    PyMuPDFLoader,\\\n    TextLoader,\\\n    YoutubeLoader,\\\n)\\\nfrom langchain_community.document_loaders.base import BaseLoader\\\nfrom langchain_core.documents import Document\\\n\\nlogger = logging.getLogger(__name__)\\\n\\nclass Crawl4AILoader(BaseLoader):\\\n    def __init__(  \\\n        self,  \\\n        url: str,  \\\n        css_selector: str | None = None,  \\\n    ) -> None:  \\\n        self.url = url  \\\n        self.css_selector = css_selector  \\\n\\n    def crawl(self, url: str, css_selector: str | None = None):  \\\n        from crawl4ai import AsyncWebCrawler  \\\n        async def _crawl():  \\\n            async with AsyncWebCrawler(verbose=True) as crawler:  \\\n                result = await crawler.arun(  \\\n                    url,  \\\n                    css_selector=css_selector or "",  \\\n                )  \\\n            return result  \\\n        import asyncio  \\\n        return asyncio.run(_crawl())\\\n\\n    def lazy_load(self) -> Iterator[Document]:  \\\n        """Load HTML document into document objects."""  \\\n        result = self.crawl(self.url, self.css_selector)  \\\n        if result.markdown is None and self.css_selector is not None:  \\\n            result = self.crawl(self.url)  \\\n        if result.markdown is None:  \\\n            raise ValueError(f"No valid content found at {self.url}")  \\\n        metadata = {  \\\n            **(result.metadata or {}),  \\\n            "source": self.url,  \\\n        }  \\\n        yield Document(page_content=result.markdown, metadata=metadata)  \\\n\\ndef get_best_loader(extract_from: str | Path) -> BaseLoader:  \\\n    match extract_from:  \\\n        case str() | Path() if os.path.isfile(extract_from):  \\\n            if os.path.splitext(extract_from)[1] == ".pdf":  \\\n                return PyMuPDFLoader(file_path=str(extract_from))  \\\n            else:  \\\n                return TextLoader(file_path=extract_from)  \\\n        case str() if extract_from.startswith("http"):  \\\n            if "youtube" in extract_from:  \\\n                video_id = YoutubeLoader.extract_video_id(extract_from)  \\\n                return YoutubeLoader(video_id=video_id)  \\\n            else:  \\\n                try:  \\\n                    return Crawl4AILoader(url=extract_from, css_selector="article")  \\\n                except Exception:  \\\n                    logger.warning(  \\\n                        dedent("""  \\\n                        Crawl4AI web loader is not available but it's recommended for  \\\n                        better results. Install `pip install neuralnoise[crawl4ai]` to  \\\n                        use it, or `pip install crawl4ai` to install it.  \\\n                        Once installed, make sure to follow the instructions in their  \\\n                        repo: https://github.com/unclecode/crawl4ai  \\\n                        For example, you should run `playwright install` to install  \\\n                        utils for the crawlers to work.  \\\n                        Using the default web loader now.  \\\n                    """)  \\\n                    html_content = requests.get(extract_from).text  \\\n                    with NamedTemporaryFile(  \\\n                        delete=False, mode="w", suffix=".html"  \\\n                    ) as f:  \\\n                        f.write(html_content)  \\\n                    f.close()  \\\n                    return BSHTMLLoader(file_path=f.name)  \\\n        case _:  \\\n            raise ValueError("Invalid input")  \\\n\\ndef extract_content_from_source(extract_from: str | Path) -> str:  \\\n    logger.info(f"Extracting content from {extract_from}")  \\\n    loader = get_best_loader(extract_from)  \\\n    docs = loader.load()  \\\n    content = """  \\\n    """  \\\n    for doc in docs:  \\\n        if doc.metadata.get("title"):  \\\n            content += f"\n\n# {doc.metadata['title']}\n\n"  \\\n        content += doc.page_content.strip()  \\\n    return content  \\\n\\ndef extract_content(  \\\n    extract_from: str | Path | list[str] | list[Path] | list[str | Path],  \\\n) -> str:  \\\n    if not isinstance(extract_from, list):  \\\n        extract_from = [extract_from]  \\\n    return "\n\n".join(  \\\n        f"<document>\n{extract_content_from_source(item)}\n</document>"  \\\n        for item in extract_from  \\\n    )  \\\n}