import logging\\\nimport os\\\nfrom pathlib import Path\\\nfrom tempfile import NamedTemporaryFile\\\nfrom textwrap import dedent\\\nfrom typing import Iterator, Optional\\\n\\nimport requests  # type: ignore\\\nfrom langchain_community.document_loaders import (  \\\\n    BSHTMLLoader,\\\\n    PyMuPDFLoader,\\\\n    TextLoader,\\\\n    YoutubeLoader,\\\\n)\\\nfrom langchain_community.document_loaders.base import BaseLoader\\\\\nfrom langchain_core.documents import Document\\\\n\\nlogger = logging.getLogger(__name__)\\\\\n\\nclass Crawl4AILoader(BaseLoader):\\\\\n    def __init__(  \\\\n        self,  \\\\n        url: str,  \\\\n        css_selector: str | None = None,  \\\\n    ) -> None:  \\\\n        self.url = url  \\\\n        self.css_selector = css_selector  \\\\n\\\\\n    def crawl(self) -> Iterator[Document]:  \\\\n        from crawl4ai import AsyncWebCrawler  \\\\n        async def _acrawl(url: str, css_selector: str | None = None) -> Document:  \\\\n            async with AsyncWebCrawler(verbose=True) as crawler:  \\\\n                result = await crawler.arun(  \\\\n                    url,  \\\\n                    css_selector=css_selector or "",  \\\\n                )  \\\\n            metadata = {  \\\\n                **(result.metadata or {}),  \\\\n                "source": self.url,  \\\\n            }  \\\\n            return Document(page_content=result.markdown, metadata=metadata)  \\\\n        import asyncio  \\\\n        return asyncio.run(_acrawl(self.url, self.css_selector))  \\\\n\\\\\n    def _process_result(self, result: Document) -> str:  \\\\n        if result.page_content is None:  \\\\n            raise ValueError("No valid content found")  \\\\n        return result.page_content  \\\\n\\\\\n    def get_best_loader(self, extract_from: str | Path) -> BaseLoader:  \\\\n        try:  \\\\n            match extract_from:  \\\\n                case str() | Path() if os.path.isfile(extract_from):  \\\\n                    if os.path.splitext(extract_from)[1] == ".pdf":  \\\\n                        return PyMuPDFLoader(file_path=str(extract_from))  \\\\n                    else:  \\\\n                        return TextLoader(file_path=extract_from)  \\\\n                case str() if extract_from.startswith("http"):  \\\\n                    if "youtube" in extract_from:  \\\\n                        video_id = YoutubeLoader.extract_video_id(extract_from)  \\\\n                        return YoutubeLoader(video_id=video_id)  \\\\n                    else:  \\\\n                        return Crawl4AILoader(url=extract_from, css_selector="article")  \\\\n        except Exception as e:  \\\\n            logger.error(f"Error loading content: {e}")  \\\\n            raise  \\\\n        logger.warning(  \\\\n            dedent("""  \\\\n            Crawl4AI web loader is not available but it's recommended for  \\\\n            better results. Install `pip install neuralnoise[crawl4ai]` to  \\\\n            use it, or `pip install crawl4ai` to install it.  \\\\n            Once installed, make sure to follow the instructions in their  \\\\n            repo: https://github.com/unclecode/crawl4ai  \\\\n            For example, you should run `playwright install` to install  \\\\n            utils for the crawlers to work.  \\\\n            Using the default web loader now.  \\\\n            """)  \\\\n        html_content = requests.get(extract_from).text  \\\\n        with NamedTemporaryFile(  \\\\n            delete=False, mode="w", suffix=".html"  \\\\n        ) as f:  \\\\n            f.write(html_content)  \\\\n        f.close()  \\\\n        return BSHTMLLoader(file_path=f.name)  \\\\n\\\\\n    def extract_content_from_source(self, extract_from: str | Path) -> str:  \\\\n        logger.info(f"Extracting content from {extract_from}")  \\\\n        loader = self.get_best_loader(extract_from)  \\\\n        docs = loader.load()  \\\\n        content_parts = []  \\\\n        for doc in docs:  \\\\n            if doc.metadata.get("title"):  \\\\n                content_parts.append(f"\n\n# {doc.metadata['title']}\n\n")  \\\\n            content_parts.append(doc.page_content.strip())  \\\\n        return "".join(content_parts)  \\\\n\\\\\n    def extract_content(  \\\\n        self,  \\\\n        extract_from: str | Path | list[str] | list[Path] | list[str | Path],  \\\\n    ) -> str:  \\\\n        if not isinstance(extract_from, list):  \\\\n            extract_from = [extract_from]  \\\\n        return "\n\n".join(  \\\\n            f"<document>\n{self.extract_content_from_source(item)}\n</document>"  \\\\n            for item in extract_from  \\\\n        )  \\\\n}